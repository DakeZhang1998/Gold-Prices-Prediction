{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Gold Future Prices Based on LSTM\n",
    "### Dake Zhang, School of Computer Science, WHU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introdution\n",
    "This is the final assignment for **Business Intelligence 2018**. A method to make predictions about the prices of Gold Futures based on **LSTM** (Long Short Term Memory network) is proposed in this experiment. As a practice of **Predictive Analytics** in the field of Business Intelligence, this method including data preprocessing, neural network training, testing and evaluation. In the experiment based on PaddlePaddle, with the advantage of LSTM, the proposed method performs well in the predictions of **Gold T+D Prices** of China.\n",
    "\n",
    "All files including the report is available at https://github.com/RickyZhang1998/Gold-Prices-Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Experiment\n",
    "### 2.1 Import related packages\n",
    "- numpy: a fundamental package for scientific computing with Python\n",
    "- pandas: an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programmin\n",
    "- matplotlib.pyplot: intended for interactive plots and simple cases of programmatic plot generatio\n",
    "- paddle.v2: An easy-to-use, easy-to-learn deep learning platform maintained by Baidu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import paddle.v2 as paddle\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocess the data\n",
    "#### 2.2.1 Load time series from CSV files and merge them\n",
    "In this experiment, we use four time series to make predictions about Gold T+D Open Prices. Note that the four time series are not fully corresponding by the date. So we choose to merge them in \"inner\" mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Open_GT  Max_GT  Min_GT  Close_GT  Open_ST  Max_ST  Min_ST  Close_ST  \\\n",
      "0      179.00  180.80  176.10    180.60     3150    3220    3081      3188   \n",
      "1      182.40  183.55  182.20    183.50     3163    3230    3163      3225   \n",
      "2      181.76  182.00  177.61    177.85     3171    3198    3107      3110   \n",
      "3      174.60  178.00  174.01    175.92     2999    3088    2978      3020   \n",
      "4      177.01  178.60  175.60    178.60     3068    3119    3035      3114   \n",
      "5      178.40  178.58  174.60    176.08     3090    3128    3041      3081   \n",
      "6      179.69  179.90  178.11    179.48     3140    3140    3080      3112   \n",
      "7      180.10  180.20  175.85    176.55     3120    3125    3040      3066   \n",
      "8      182.60  183.88  182.20    182.75     2804    2804    2804      2804   \n",
      "9      188.21  192.53  188.21    191.00     2808    2841    2776      2824   \n",
      "10     194.12  197.20  191.20    196.00     2877    2895    2815      2880   \n",
      "11     200.48  200.48  194.00    194.42     2921    2938    2858      2869   \n",
      "12     194.50  202.35  193.50    202.29     2861    3000    2853      2982   \n",
      "13     188.45  190.75  188.05    189.65     2731    2731    2731      2731   \n",
      "14     187.01  188.45  182.60    185.81     2665    2738    2630      2691   \n",
      "15     185.60  188.00  184.00    186.42     2699    2720    2655      2703   \n",
      "16     184.49  188.00  183.00    183.90     2650    2695    2603      2615   \n",
      "17     183.80  184.30  173.80    179.10     2630    2635    2470      2503   \n",
      "18     176.40  178.20  175.25    177.92     2518    2518    2463      2480   \n",
      "19     167.50  167.90  159.10    161.26     2420    2466    2338      2355   \n",
      "20     162.85  163.85  161.50    163.63     2398    2400    2365      2380   \n",
      "21     163.03  167.40  162.60    164.50     2300    2345    2280      2297   \n",
      "22     177.60  181.20  177.23    178.75     2389    2433    2381      2403   \n",
      "23     178.20  180.30  177.81    178.61     2409    2418    2395      2409   \n",
      "24     178.61  179.29  178.22    179.04     2414    2417    2408      2415   \n",
      "25     178.80  179.05  177.70    178.09     2405    2410    2396      2399   \n",
      "26     175.02  175.98  169.85    170.10     2349    2367    2282      2286   \n",
      "27     172.49  173.69  171.60    173.10     2304    2334    2302      2308   \n",
      "28     171.80  172.60  170.20    171.17     2300    2317    2280      2302   \n",
      "29     170.00  173.21  169.88    172.16     2293    2320    2290      2315   \n",
      "...       ...     ...     ...       ...      ...     ...     ...       ...   \n",
      "2110   276.69  277.44  276.18    277.28     3591    3597    3574      3581   \n",
      "2111   277.32  278.85  276.66    276.85     3581    3599    3574      3577   \n",
      "2112   276.65  277.05  275.77    275.79     3580    3591    3543      3545   \n",
      "2113   275.15  275.98  273.80    273.80     3540    3550    3525      3531   \n",
      "2114   274.19  274.73  273.13    274.68     3527    3530    3504      3523   \n",
      "2115   274.95  275.89  274.28    274.49     3530    3566    3529      3556   \n",
      "2116   274.10  275.26  273.71    275.25     3566    3580    3542      3567   \n",
      "2117   275.85  275.85  274.45    274.88     3571    3571    3554      3562   \n",
      "2118   275.60  275.85  274.10    275.25     3568    3571    3542      3557   \n",
      "2119   275.11  275.45  273.60    273.81     3565    3565    3543      3548   \n",
      "2120   273.56  274.40  273.30    273.47     3543    3551    3524      3531   \n",
      "2121   273.41  273.64  271.01    271.29     3525    3530    3495      3501   \n",
      "2122   271.20  271.60  270.17    270.44     3497    3500    3484      3490   \n",
      "2123   269.70  270.60  269.51    269.89     3481    3496    3470      3475   \n",
      "2124   269.80  271.77  269.49    271.67     3470    3506    3464      3504   \n",
      "2125   271.30  272.45  270.71    272.36     3497    3519    3491      3514   \n",
      "2126   272.60  273.90  272.53    273.38     3515    3529    3509      3519   \n",
      "2127   273.61  274.49  273.28    274.45     3522    3527    3516      3525   \n",
      "2128   274.43  275.17  273.72    274.15     3527    3537    3497      3514   \n",
      "2129   274.21  274.98  273.80    274.60     3520    3534    3515      3526   \n",
      "2130   274.80  275.08  274.45    274.49     3531    3531    3510      3511   \n",
      "2131   274.10  275.00  273.92    274.82     3506    3509    3495      3507   \n",
      "2132   274.88  274.89  274.07    274.15     3505    3508    3482      3482   \n",
      "2133   274.47  274.80  272.46    272.72     3491    3497    3471      3476   \n",
      "2134   272.90  274.99  272.48    274.65     3481    3512    3471      3505   \n",
      "2135   274.65  275.17  274.21    274.53     3501    3509    3497      3498   \n",
      "2136   274.32  274.49  273.46    274.07     3494    3503    3472      3501   \n",
      "2137   274.00  274.61  273.66    273.85     3500    3513    3497      3503   \n",
      "2138   274.10  274.42  273.60    274.09     3510    3518    3499      3500   \n",
      "2139   274.22  275.60  274.10    275.14     3505    3511    3500      3501   \n",
      "\n",
      "      Open_LG   Max_LG   Min_LG  Close_LG  Open_LS  Max_LS  Min_LS  Close_LS  \n",
      "0      812.28   839.47   811.27    835.87   13.264  13.885  13.208    13.857  \n",
      "1      830.80   836.78   815.85    816.40   13.627  13.826  13.350    13.452  \n",
      "2      816.36   820.33   790.65    803.97   13.434  13.505  12.480    13.067  \n",
      "3      804.06   808.92   789.45    801.15   13.069  13.153  12.570    12.917  \n",
      "4      801.18   815.08   793.88    796.10   12.920  13.200  12.640    12.800  \n",
      "5      796.10   819.60   790.30    802.26   12.805  13.125  12.080    12.200  \n",
      "6      803.80   818.67   796.97    801.33   12.287  12.745  11.907    12.072  \n",
      "7      801.40   805.38   776.76    777.42   12.075  12.210  11.310    11.365  \n",
      "8      836.95   876.61   825.70    858.87   11.097  11.515  10.890    10.930  \n",
      "9      858.89   891.63   856.46    888.45   10.922  11.877  10.917    11.527  \n",
      "10     888.41   922.10   878.03    907.41   11.523  12.096  11.403    11.785  \n",
      "11     907.58   918.60   880.55    910.50   11.788  12.130  11.498    12.015  \n",
      "12     910.35   931.85   827.80    846.70   12.015  12.340   9.380     9.940  \n",
      "13     847.60   872.75   822.22    834.13   10.205  10.880  10.050    10.730  \n",
      "14     834.18   854.65   830.80    837.40   10.735  11.180  10.577    10.990  \n",
      "15     837.42   856.85   831.05    846.03   10.987  11.100  10.028    10.390  \n",
      "16     846.39   850.58   784.90    805.97   10.382  10.390   9.248     9.720  \n",
      "17     806.10   815.00   772.88    785.42    9.722   9.942   9.097     9.400  \n",
      "18     782.90   810.67   781.80    795.22    9.412   9.958   9.395     9.830  \n",
      "19     725.63   735.62   698.43    718.07    9.483   9.923   9.208     9.490  \n",
      "20     733.14   739.60   709.25    711.40    9.799   9.920   9.278     9.340  \n",
      "21     735.13   753.50   733.07    745.45    9.269   9.483   8.827     9.020  \n",
      "22     821.18   823.45   808.63    817.00   10.328  10.491  10.175    10.360  \n",
      "23     817.12   817.88   809.84    815.55   10.355  10.405  10.205    10.360  \n",
      "24     815.38   820.97   809.79    818.59   10.340  10.395  10.110    10.310  \n",
      "25     814.00   819.30   768.40    770.60   10.300  10.330   9.185     9.297  \n",
      "26     770.61   787.33   762.45    781.40    9.298   9.745   9.152     9.605  \n",
      "27     781.29   783.93   764.08    773.48    9.604   9.755   9.300     9.642  \n",
      "28     773.53   785.06   762.73    766.97    9.643   9.733   9.351     9.538  \n",
      "29     767.06   773.39   741.26    757.92    9.536   9.581   9.151     9.512  \n",
      "...       ...      ...      ...       ...      ...     ...     ...       ...  \n",
      "2110  1231.81  1243.06  1223.10   1233.47   14.634  14.769  14.550    14.687  \n",
      "2111  1233.02  1242.10  1220.80   1229.96   14.677  14.755  14.399    14.470  \n",
      "2112  1230.14  1230.53  1219.40   1222.42   14.488  14.810  14.350    14.446  \n",
      "2113  1222.60  1222.73  1211.70   1215.96   14.462  14.467  14.030    14.263  \n",
      "2114  1216.19  1237.27  1215.40   1232.80   14.262  14.950  14.260    14.727  \n",
      "2115  1232.60  1236.25  1229.60   1232.72   14.743  14.905  14.620    14.710  \n",
      "2116  1234.06  1235.03  1226.40   1230.92   14.747  14.764  14.580    14.628  \n",
      "2117  1231.29  1235.90  1223.20   1227.59   14.646  14.707  14.470    14.525  \n",
      "2118  1227.84  1236.21  1222.90   1225.92   14.542  14.717  14.460    14.550  \n",
      "2119  1226.03  1226.67  1219.80   1222.95   14.566  14.588  14.360    14.417  \n",
      "2120  1223.30  1223.30  1206.20   1209.82   14.419  14.436  14.080    14.123  \n",
      "2121  1210.81  1211.44  1199.70   1202.16   14.186  14.210  13.927    14.009  \n",
      "2122  1202.00  1205.17  1196.00   1202.89   14.008  14.119  13.930    14.000  \n",
      "2123  1203.13  1216.04  1197.20   1210.91   14.008  14.179  13.870    14.123  \n",
      "2124  1210.70  1216.09  1207.70   1213.81   14.137  14.343  14.070    14.274  \n",
      "2125  1213.98  1225.14  1213.50   1221.36   14.288  14.418  14.220    14.373  \n",
      "2126  1222.57  1233.40  1217.60   1223.95   14.413  14.436  14.330    14.423  \n",
      "2127  1223.60  1228.40  1219.60   1221.30   14.407  14.486  14.210    14.304  \n",
      "2128  1221.70  1229.84  1219.30   1225.58   14.304  14.553  14.250    14.471  \n",
      "2129  1225.20  1229.00  1222.10   1227.23   14.472  14.539  14.390    14.486  \n",
      "2130  1227.45  1228.60  1219.80   1222.85   14.486  14.503  14.170    14.264  \n",
      "2131  1224.43  1231.40  1221.20   1222.51   14.304  14.434  14.181    14.232  \n",
      "2132  1222.40  1231.40  1211.40   1214.36   14.231  14.301  14.070    14.138  \n",
      "2133  1214.10  1226.00  1210.60   1220.93   14.139  14.381  14.090    14.306  \n",
      "2134  1221.16  1228.84  1220.20   1224.12   14.321  14.381  14.240    14.295  \n",
      "2135  1224.31  1225.52  1216.30   1222.09   14.295  14.336  14.030    14.162  \n",
      "2136  1222.16  1236.90  1220.80   1231.06   14.233  14.562  14.191    14.386  \n",
      "2137  1231.24  1241.81  1230.10   1238.57   14.386  14.655  14.360    14.515  \n",
      "2138  1238.76  1239.07  1232.40   1237.60   14.532  14.538  14.390    14.477  \n",
      "2139  1238.23  1240.76  1234.30   1235.55   14.496  14.505  14.340    14.375  \n",
      "\n",
      "[2140 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "gold_td = pd.read_csv('/home/aistudio/data/data2233/GoldT+D.csv', header=0, names=['Date', 'Open_GT', 'Max_GT', 'Min_GT', 'Close_GT'])\n",
    "silver_td = pd.read_csv('/home/aistudio/data/data2233/SilverT+D.csv', header=0, names=['Date', 'Open_ST', 'Max_ST', 'Min_ST', 'Close_ST'])\n",
    "London_gold = pd.read_csv('/home/aistudio/data/data2233/LondonGold.csv', header=0, names=['Date', 'Open_LG', 'Max_LG', 'Min_LG', 'Close_LG'])\n",
    "London_silver = pd.read_csv('/home/aistudio/data/data2233/LondonSilver.csv', header=0, names=['Date', 'Open_LS', 'Max_LS', 'Min_LS', 'Close_LS'])\n",
    "\n",
    "csv_data = pd.merge(gold_td, silver_td, on='Date', how='inner')\n",
    "csv_data = pd.merge(csv_data, London_gold, on='Date', how='inner')\n",
    "csv_data = pd.merge(csv_data, London_silver, on='Date', how='inner')\n",
    "\n",
    "#The \"Date\" column is useless, so we drop it.\n",
    "csv_data = csv_data.drop([\"Date\"], axis=1)\n",
    "\n",
    "# Check the data.\n",
    "print(csv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Chang the form of data\n",
    "- We transfer csv_data of type \"DataFrame\" into array and add the label, namely the open price of the next day. \n",
    "- Then we reshape the array into 2139 x 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[179.    180.8   176.1   ...  13.208  13.857 182.4  ]\n",
      " [182.4   183.55  182.2   ...  13.35   13.452 181.76 ]\n",
      " [181.76  182.    177.61  ...  12.48   13.067 174.6  ]\n",
      " ...\n",
      " [274.32  274.49  273.46  ...  14.191  14.386 274.   ]\n",
      " [274.    274.61  273.66  ...  14.36   14.515 274.1  ]\n",
      " [274.1   274.42  273.6   ...  14.39   14.477 274.22 ]]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(csv_data.shape[0] - 1):  \n",
    "    line = csv_data[i:i+1]\n",
    "    for j in range(line.shape[1]):\n",
    "        data.append(float(line.iat[0, j]))\n",
    "    data.append(float(csv_data[i+1:i+2][\"Open_GT\"]))\n",
    "\n",
    "data = np.array(data)\n",
    "data = data.reshape(2139, 17)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Define functions to get the Training Set and Test Set \n",
    "- We use ratio 7:3 to separate the training set and test set. That's first 1497 records as training set and last 642 records as test set.\n",
    "- We apply normalization to each feature to improve the accuracy and speed up the convergence of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Training Set.\n",
    "def get_train_data(train_number):\n",
    "    data_train = data[0:train_number]\n",
    "    maximums = data_train.max(axis=0)\n",
    "    minimums = data_train.min(axis=0)\n",
    "    avgs = data_train.sum(axis=0) / data_train.shape[0]\n",
    "    normalized_train_data = (data_train - avgs) / (maximums - minimums)\n",
    "    \n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    train_x = normalized_train_data[0:train_number, :data.shape[1] - 1]\n",
    "    train_y = normalized_train_data[0:train_number, data.shape[1] - 1, np.newaxis]\n",
    "    \n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Test Set.\n",
    "def get_test_data(train_number):\n",
    "    data_test = data[train_number:]\n",
    "    maximums = data_test.max(axis=0)\n",
    "    minimums = data_test.min(axis=0)\n",
    "    avgs = data_test.sum(axis=0) / data_test.shape[0]\n",
    "    normalized_test_data = (data_test - avgs) / (maximums - minimums)\n",
    "    \n",
    "    test_x = []\n",
    "    test_y = [] \n",
    "    test_x = normalized_test_data[0:train_number, :data.shape[1] - 1]\n",
    "    test_y = normalized_test_data[0:train_number, data.shape[1] - 1, np.newaxis]\n",
    "    \n",
    "    return test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define reader\n",
    "- ```train_reader()``` function is used to read Training Set and Test Set.\n",
    "- Within ```read_data()```, we use ```yield``` to make ```reader()``` a **Generator**.\n",
    "- Advantage: reduce the pressure on memory. Since we sometimes cannot load the whole data set into memory at once which will be a great waste of memory, this generator will just get the data in need.\n",
    "- In this case, we get 20 records every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reader(train_x, train_y):\n",
    "    def reader():\n",
    "        for n in xrange(train_x.shape[0] - 20):\n",
    "            yield train_x[n:n+20], train_y[n:n+20]\n",
    "    return reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Design the LSTM model\n",
    "#### 2.4.1 Initialize the PaddlePaddle\n",
    "In this case, we don't use GPU to do the training and we set the trainer to be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddle.init(use_gpu=False, trainer_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Define the input layer\n",
    "We use time series as input and define the input as 16-dimension vector sequences while the 17th dimension is the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = paddle.layer.data(name=\"x\", type=paddle.data_type.dense_vector_sequence(16))        # define input as 16-dimension vector sequences\n",
    "x_to = paddle.layer.fc(input=x, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Define the output layer\n",
    "We use function ```paddle.layer.lstmmemory(input=image, size=1, act=paddle.activation.Tanh())``` to define a LSTM layer. The input is ```x_to```, which is defined above. The number of neuron is set to be 1. And the activation function is defined to be ```Tanh()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING 2018-12-14 16:28:49,473 layers.py:1579] size of lstmemory layer: __lstmemory_0__ is automatically set to size of input layer / 4. The parameter size passing to this layer is ignored.\n"
     ]
    }
   ],
   "source": [
    "y_predict = paddle.layer.lstmemory(input=x_to, size=1, act=paddle.activation.Tanh())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.4 Define the label layer\n",
    "This step is similar to the step that defines the input layer, except that we define the label to be 1 dimension value sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = paddle.layer.data(name='y', type=paddle.data_type.dense_vector_sequence(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.5 Define the cost function\n",
    "In this case, we use **MSE (Mean Square Error)** function to be the cost function to calculate the gradient and to refine the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = paddle.layer.square_error_cost(input=y_predict, label=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.6 Create parameters\n",
    "Function ```paddle.parameters.create()``` is used to create and initialize parameters. In this case, we use cost function defined above to create and initialize parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = paddle.parameters.create(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.7 Create the optimizer\n",
    "In this case, we use Adam optimizer provided by ```paddle.optimizer.Adam(learning_rate=0.003,regularization=paddle.optimizer.L2Regularization(rate=0.01))``` as the optimizer. We set the learning rate to be 0.003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = paddle.optimizer.Adam(learning_rate=0.003, regularization=paddle.optimizer.L2Regularization(rate=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.8 Define the map feeding\n",
    "This feeding is a map from the name of the layer to the index of the array to input data while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeding = {'x': 0, 'y': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.9 Define the event handler\n",
    "This event handler is defined to inform us the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_handler(event):\n",
    "    if isinstance(event, paddle.event.EndIteration):\n",
    "        if event.batch_id % 100 == 0:\n",
    "            print(\"Pass %d, Batch %d, Cost %f\" % (\n",
    "                    event.pass_id, event.batch_id, event.cost))\n",
    "    if isinstance(event, paddle.event.EndPass):\n",
    "        result = trainer.test(\n",
    "               reader=paddle.batch(\n",
    "                train_reader(train_x, train_y), batch_size=4),\n",
    "                feeding=feeding)\n",
    "        print(\"Test %d, Cost %f\" % (event.pass_id, result.cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Start training\n",
    "#### 2.5.1 Create a trainer\n",
    "We use ```paddle.trainer.SGD()``` to define a  Stochastic Gradient Descent trainer with cost function, parameters and optimizer all defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = paddle.trainer.SGD(cost=cost, parameters=parameters, update_equation=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 Set training parameters and start training\n",
    "- We use ```paddle.reader.shuffle(train(), buf_size=60)``` to represent that the trainer reads data records of ```buf_size``` and shuffle them.\n",
    "- We use ```paddle.batch(reader(), batch_size=20``` to take data records of ```batch_size``` from data records above to do one time training).\n",
    "- We use ```feeding``` to be the source of training data. \n",
    "- ```event_handler``` is defined above.\n",
    "- ```num_passes``` represents the number of times of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 0, Batch 0, Cost 2.472466\n",
      "Test 0, Cost 0.185731\n",
      "Pass 1, Batch 0, Cost 0.662272\n",
      "Test 1, Cost 0.053168\n",
      "Pass 2, Batch 0, Cost 0.315328\n",
      "Test 2, Cost 0.046647\n",
      "Pass 3, Batch 0, Cost 0.205391\n",
      "Test 3, Cost 0.043295\n",
      "Pass 4, Batch 0, Cost 0.214116\n",
      "Test 4, Cost 0.039944\n",
      "Pass 5, Batch 0, Cost 0.191567\n",
      "Test 5, Cost 0.036684\n",
      "Pass 6, Batch 0, Cost 0.175632\n",
      "Test 6, Cost 0.033504\n",
      "Pass 7, Batch 0, Cost 0.148006\n",
      "Test 7, Cost 0.030395\n",
      "Pass 8, Batch 0, Cost 0.118826\n",
      "Test 8, Cost 0.027460\n",
      "Pass 9, Batch 0, Cost 0.156407\n",
      "Test 9, Cost 0.024750\n",
      "Pass 10, Batch 0, Cost 0.162004\n",
      "Test 10, Cost 0.022347\n",
      "Pass 11, Batch 0, Cost 0.099139\n",
      "Test 11, Cost 0.020351\n",
      "Pass 12, Batch 0, Cost 0.135527\n",
      "Test 12, Cost 0.018795\n",
      "Pass 13, Batch 0, Cost 0.118409\n",
      "Test 13, Cost 0.017649\n",
      "Pass 14, Batch 0, Cost 0.126628\n",
      "Test 14, Cost 0.016805\n",
      "Pass 15, Batch 0, Cost 0.124971\n",
      "Test 15, Cost 0.016199\n",
      "Pass 16, Batch 0, Cost 0.095086\n",
      "Test 16, Cost 0.015678\n",
      "Pass 17, Batch 0, Cost 0.125245\n",
      "Test 17, Cost 0.015236\n",
      "Pass 18, Batch 0, Cost 0.078791\n",
      "Test 18, Cost 0.014723\n",
      "Pass 19, Batch 0, Cost 0.106777\n",
      "Test 19, Cost 0.014316\n",
      "Pass 20, Batch 0, Cost 0.098376\n",
      "Test 20, Cost 0.013838\n",
      "Pass 21, Batch 0, Cost 0.093407\n",
      "Test 21, Cost 0.013462\n",
      "Pass 22, Batch 0, Cost 0.080060\n",
      "Test 22, Cost 0.013015\n",
      "Pass 23, Batch 0, Cost 0.091529\n",
      "Test 23, Cost 0.012619\n",
      "Pass 24, Batch 0, Cost 0.072659\n",
      "Test 24, Cost 0.012205\n",
      "Pass 25, Batch 0, Cost 0.079428\n",
      "Test 25, Cost 0.011842\n",
      "Pass 26, Batch 0, Cost 0.091540\n",
      "Test 26, Cost 0.011517\n",
      "Pass 27, Batch 0, Cost 0.085512\n",
      "Test 27, Cost 0.011149\n",
      "Pass 28, Batch 0, Cost 0.067982\n",
      "Test 28, Cost 0.010846\n",
      "Pass 29, Batch 0, Cost 0.087085\n",
      "Test 29, Cost 0.010558\n",
      "Pass 30, Batch 0, Cost 0.067652\n",
      "Test 30, Cost 0.010228\n",
      "Pass 31, Batch 0, Cost 0.072019\n",
      "Test 31, Cost 0.009966\n",
      "Pass 32, Batch 0, Cost 0.067026\n",
      "Test 32, Cost 0.009730\n",
      "Pass 33, Batch 0, Cost 0.077878\n",
      "Test 33, Cost 0.009465\n",
      "Pass 34, Batch 0, Cost 0.064369\n",
      "Test 34, Cost 0.009228\n",
      "Pass 35, Batch 0, Cost 0.063322\n",
      "Test 35, Cost 0.009052\n",
      "Pass 36, Batch 0, Cost 0.068750\n",
      "Test 36, Cost 0.008822\n",
      "Pass 37, Batch 0, Cost 0.057476\n",
      "Test 37, Cost 0.008648\n",
      "Pass 38, Batch 0, Cost 0.052738\n",
      "Test 38, Cost 0.008394\n",
      "Pass 39, Batch 0, Cost 0.065212\n",
      "Test 39, Cost 0.008300\n",
      "Pass 40, Batch 0, Cost 0.062410\n",
      "Test 40, Cost 0.008185\n",
      "Pass 41, Batch 0, Cost 0.051300\n",
      "Test 41, Cost 0.008039\n",
      "Pass 42, Batch 0, Cost 0.054374\n",
      "Test 42, Cost 0.007872\n",
      "Pass 43, Batch 0, Cost 0.051074\n",
      "Test 43, Cost 0.007787\n",
      "Pass 44, Batch 0, Cost 0.043278\n",
      "Test 44, Cost 0.007692\n",
      "Pass 45, Batch 0, Cost 0.045289\n",
      "Test 45, Cost 0.007614\n",
      "Pass 46, Batch 0, Cost 0.050721\n",
      "Test 46, Cost 0.007535\n",
      "Pass 47, Batch 0, Cost 0.047780\n",
      "Test 47, Cost 0.007494\n",
      "Pass 48, Batch 0, Cost 0.044589\n",
      "Test 48, Cost 0.007393\n",
      "Pass 49, Batch 0, Cost 0.043011\n",
      "Test 49, Cost 0.007309\n",
      "Pass 50, Batch 0, Cost 0.044078\n",
      "Test 50, Cost 0.007309\n",
      "Pass 51, Batch 0, Cost 0.033164\n",
      "Test 51, Cost 0.007221\n",
      "Pass 52, Batch 0, Cost 0.037538\n",
      "Test 52, Cost 0.007262\n",
      "Pass 53, Batch 0, Cost 0.033414\n",
      "Test 53, Cost 0.007184\n",
      "Pass 54, Batch 0, Cost 0.038779\n",
      "Test 54, Cost 0.007135\n",
      "Pass 55, Batch 0, Cost 0.030736\n",
      "Test 55, Cost 0.007075\n",
      "Pass 56, Batch 0, Cost 0.031764\n",
      "Test 56, Cost 0.007170\n",
      "Pass 57, Batch 0, Cost 0.039284\n",
      "Test 57, Cost 0.007348\n",
      "Pass 58, Batch 0, Cost 0.029778\n",
      "Test 58, Cost 0.007249\n",
      "Pass 59, Batch 0, Cost 0.032196\n",
      "Test 59, Cost 0.007011\n",
      "Pass 60, Batch 0, Cost 0.029850\n",
      "Test 60, Cost 0.006949\n",
      "Pass 61, Batch 0, Cost 0.037958\n",
      "Test 61, Cost 0.007346\n",
      "Pass 62, Batch 0, Cost 0.036700\n",
      "Test 62, Cost 0.007561\n",
      "Pass 63, Batch 0, Cost 0.028816\n",
      "Test 63, Cost 0.007207\n",
      "Pass 64, Batch 0, Cost 0.034777\n",
      "Test 64, Cost 0.006986\n",
      "Pass 65, Batch 0, Cost 0.030801\n",
      "Test 65, Cost 0.007055\n",
      "Pass 66, Batch 0, Cost 0.027218\n",
      "Test 66, Cost 0.007263\n",
      "Pass 67, Batch 0, Cost 0.030260\n",
      "Test 67, Cost 0.007559\n",
      "Pass 68, Batch 0, Cost 0.026289\n",
      "Test 68, Cost 0.007405\n",
      "Pass 69, Batch 0, Cost 0.029539\n",
      "Test 69, Cost 0.007209\n",
      "Pass 70, Batch 0, Cost 0.025409\n",
      "Test 70, Cost 0.007106\n",
      "Pass 71, Batch 0, Cost 0.027699\n",
      "Test 71, Cost 0.007315\n",
      "Pass 72, Batch 0, Cost 0.023804\n",
      "Test 72, Cost 0.007383\n",
      "Pass 73, Batch 0, Cost 0.026747\n",
      "Test 73, Cost 0.007489\n",
      "Pass 74, Batch 0, Cost 0.023950\n",
      "Test 74, Cost 0.007368\n",
      "Pass 75, Batch 0, Cost 0.030243\n",
      "Test 75, Cost 0.007344\n",
      "Pass 76, Batch 0, Cost 0.026837\n",
      "Test 76, Cost 0.007288\n",
      "Pass 77, Batch 0, Cost 0.027655\n",
      "Test 77, Cost 0.007417\n",
      "Pass 78, Batch 0, Cost 0.026069\n",
      "Test 78, Cost 0.007348\n",
      "Pass 79, Batch 0, Cost 0.024858\n",
      "Test 79, Cost 0.007161\n",
      "Pass 80, Batch 0, Cost 0.027914\n",
      "Test 80, Cost 0.007297\n",
      "Pass 81, Batch 0, Cost 0.022543\n",
      "Test 81, Cost 0.007287\n",
      "Pass 82, Batch 0, Cost 0.023850\n",
      "Test 82, Cost 0.007234\n",
      "Pass 83, Batch 0, Cost 0.023198\n",
      "Test 83, Cost 0.007344\n",
      "Pass 84, Batch 0, Cost 0.026745\n",
      "Test 84, Cost 0.007470\n",
      "Pass 85, Batch 0, Cost 0.025353\n",
      "Test 85, Cost 0.007358\n",
      "Pass 86, Batch 0, Cost 0.023248\n",
      "Test 86, Cost 0.007238\n",
      "Pass 87, Batch 0, Cost 0.023708\n",
      "Test 87, Cost 0.007203\n",
      "Pass 88, Batch 0, Cost 0.026364\n",
      "Test 88, Cost 0.007338\n",
      "Pass 89, Batch 0, Cost 0.021696\n",
      "Test 89, Cost 0.007230\n",
      "Pass 90, Batch 0, Cost 0.022435\n",
      "Test 90, Cost 0.007148\n",
      "Pass 91, Batch 0, Cost 0.023623\n",
      "Test 91, Cost 0.007122\n",
      "Pass 92, Batch 0, Cost 0.019476\n",
      "Test 92, Cost 0.006976\n",
      "Pass 93, Batch 0, Cost 0.021924\n",
      "Test 93, Cost 0.007136\n",
      "Pass 94, Batch 0, Cost 0.022432\n",
      "Test 94, Cost 0.007247\n",
      "Pass 95, Batch 0, Cost 0.024136\n",
      "Test 95, Cost 0.007314\n",
      "Pass 96, Batch 0, Cost 0.024742\n",
      "Test 96, Cost 0.007261\n",
      "Pass 97, Batch 0, Cost 0.022714\n",
      "Test 97, Cost 0.007286\n",
      "Pass 98, Batch 0, Cost 0.024584\n",
      "Test 98, Cost 0.007313\n",
      "Pass 99, Batch 0, Cost 0.023209\n",
      "Test 99, Cost 0.007283\n",
      "Pass 100, Batch 0, Cost 0.021515\n",
      "Test 100, Cost 0.007005\n",
      "Pass 101, Batch 0, Cost 0.021864\n",
      "Test 101, Cost 0.007035\n",
      "Pass 102, Batch 0, Cost 0.021253\n",
      "Test 102, Cost 0.006944\n",
      "Pass 103, Batch 0, Cost 0.021088\n",
      "Test 103, Cost 0.006978\n",
      "Pass 104, Batch 0, Cost 0.021079\n",
      "Test 104, Cost 0.007040\n",
      "Pass 105, Batch 0, Cost 0.019580\n",
      "Test 105, Cost 0.006991\n",
      "Pass 106, Batch 0, Cost 0.019343\n",
      "Test 106, Cost 0.006957\n",
      "Pass 107, Batch 0, Cost 0.020913\n",
      "Test 107, Cost 0.007093\n",
      "Pass 108, Batch 0, Cost 0.020916\n",
      "Test 108, Cost 0.007073\n",
      "Pass 109, Batch 0, Cost 0.020900\n",
      "Test 109, Cost 0.007194\n",
      "Pass 110, Batch 0, Cost 0.021362\n",
      "Test 110, Cost 0.007145\n",
      "Pass 111, Batch 0, Cost 0.019480\n",
      "Test 111, Cost 0.007133\n",
      "Pass 112, Batch 0, Cost 0.022210\n",
      "Test 112, Cost 0.007313\n",
      "Pass 113, Batch 0, Cost 0.020159\n",
      "Test 113, Cost 0.007215\n",
      "Pass 114, Batch 0, Cost 0.019709\n",
      "Test 114, Cost 0.007184\n",
      "Pass 115, Batch 0, Cost 0.020244\n",
      "Test 115, Cost 0.007076\n",
      "Pass 116, Batch 0, Cost 0.022505\n",
      "Test 116, Cost 0.007105\n",
      "Pass 117, Batch 0, Cost 0.019313\n",
      "Test 117, Cost 0.006938\n",
      "Pass 118, Batch 0, Cost 0.020467\n",
      "Test 118, Cost 0.006952\n",
      "Pass 119, Batch 0, Cost 0.017968\n",
      "Test 119, Cost 0.006743\n",
      "Pass 120, Batch 0, Cost 0.017399\n",
      "Test 120, Cost 0.006686\n",
      "Pass 121, Batch 0, Cost 0.019050\n",
      "Test 121, Cost 0.006691\n",
      "Pass 122, Batch 0, Cost 0.021182\n",
      "Test 122, Cost 0.006668\n",
      "Pass 123, Batch 0, Cost 0.018577\n",
      "Test 123, Cost 0.006673\n",
      "Pass 124, Batch 0, Cost 0.020183\n",
      "Test 124, Cost 0.006790\n",
      "Pass 125, Batch 0, Cost 0.019980\n",
      "Test 125, Cost 0.006718\n",
      "Pass 126, Batch 0, Cost 0.017944\n",
      "Test 126, Cost 0.006773\n",
      "Pass 127, Batch 0, Cost 0.018976\n",
      "Test 127, Cost 0.006867\n",
      "Pass 128, Batch 0, Cost 0.020915\n",
      "Test 128, Cost 0.006946\n",
      "Pass 129, Batch 0, Cost 0.019512\n",
      "Test 129, Cost 0.006980\n",
      "Pass 130, Batch 0, Cost 0.017298\n",
      "Test 130, Cost 0.007087\n",
      "Pass 131, Batch 0, Cost 0.016607\n",
      "Test 131, Cost 0.007053\n",
      "Pass 132, Batch 0, Cost 0.017202\n",
      "Test 132, Cost 0.006996\n",
      "Pass 133, Batch 0, Cost 0.019103\n",
      "Test 133, Cost 0.007064\n",
      "Pass 134, Batch 0, Cost 0.016627\n",
      "Test 134, Cost 0.006937\n",
      "Pass 135, Batch 0, Cost 0.017219\n",
      "Test 135, Cost 0.006979\n",
      "Pass 136, Batch 0, Cost 0.018323\n",
      "Test 136, Cost 0.006849\n",
      "Pass 137, Batch 0, Cost 0.015929\n",
      "Test 137, Cost 0.006800\n",
      "Pass 138, Batch 0, Cost 0.017454\n",
      "Test 138, Cost 0.006737\n",
      "Pass 139, Batch 0, Cost 0.017674\n",
      "Test 139, Cost 0.006532\n",
      "Pass 140, Batch 0, Cost 0.016411\n",
      "Test 140, Cost 0.006495\n",
      "Pass 141, Batch 0, Cost 0.017110\n",
      "Test 141, Cost 0.006316\n",
      "Pass 142, Batch 0, Cost 0.019421\n",
      "Test 142, Cost 0.006336\n",
      "Pass 143, Batch 0, Cost 0.015952\n",
      "Test 143, Cost 0.006213\n",
      "Pass 144, Batch 0, Cost 0.017972\n",
      "Test 144, Cost 0.006178\n",
      "Pass 145, Batch 0, Cost 0.016145\n",
      "Test 145, Cost 0.006135\n",
      "Pass 146, Batch 0, Cost 0.016550\n",
      "Test 146, Cost 0.006158\n",
      "Pass 147, Batch 0, Cost 0.017779\n",
      "Test 147, Cost 0.006216\n",
      "Pass 148, Batch 0, Cost 0.018322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 148, Cost 0.006398\n",
      "Pass 149, Batch 0, Cost 0.018240\n",
      "Test 149, Cost 0.006521\n",
      "Pass 150, Batch 0, Cost 0.018019\n",
      "Test 150, Cost 0.006721\n",
      "Pass 151, Batch 0, Cost 0.015228\n",
      "Test 151, Cost 0.006869\n",
      "Pass 152, Batch 0, Cost 0.016603\n",
      "Test 152, Cost 0.007149\n",
      "Pass 153, Batch 0, Cost 0.016842\n",
      "Test 153, Cost 0.007332\n",
      "Pass 154, Batch 0, Cost 0.015410\n",
      "Test 154, Cost 0.007540\n",
      "Pass 155, Batch 0, Cost 0.017039\n",
      "Test 155, Cost 0.007812\n",
      "Pass 156, Batch 0, Cost 0.016611\n",
      "Test 156, Cost 0.007569\n",
      "Pass 157, Batch 0, Cost 0.015274\n",
      "Test 157, Cost 0.007407\n",
      "Pass 158, Batch 0, Cost 0.016505\n",
      "Test 158, Cost 0.007129\n",
      "Pass 159, Batch 0, Cost 0.016354\n",
      "Test 159, Cost 0.006660\n",
      "Pass 160, Batch 0, Cost 0.015334\n",
      "Test 160, Cost 0.006307\n",
      "Pass 161, Batch 0, Cost 0.016123\n",
      "Test 161, Cost 0.006057\n",
      "Pass 162, Batch 0, Cost 0.017016\n",
      "Test 162, Cost 0.005779\n",
      "Pass 163, Batch 0, Cost 0.016303\n",
      "Test 163, Cost 0.005594\n",
      "Pass 164, Batch 0, Cost 0.014975\n",
      "Test 164, Cost 0.005503\n",
      "Pass 165, Batch 0, Cost 0.016469\n",
      "Test 165, Cost 0.005370\n",
      "Pass 166, Batch 0, Cost 0.015857\n",
      "Test 166, Cost 0.005369\n",
      "Pass 167, Batch 0, Cost 0.016992\n",
      "Test 167, Cost 0.005249\n",
      "Pass 168, Batch 0, Cost 0.016412\n",
      "Test 168, Cost 0.005239\n",
      "Pass 169, Batch 0, Cost 0.017039\n",
      "Test 169, Cost 0.005152\n",
      "Pass 170, Batch 0, Cost 0.016739\n",
      "Test 170, Cost 0.005142\n",
      "Pass 171, Batch 0, Cost 0.015684\n",
      "Test 171, Cost 0.005119\n",
      "Pass 172, Batch 0, Cost 0.016390\n",
      "Test 172, Cost 0.005253\n",
      "Pass 173, Batch 0, Cost 0.016863\n",
      "Test 173, Cost 0.005553\n",
      "Pass 174, Batch 0, Cost 0.017801\n",
      "Test 174, Cost 0.006078\n",
      "Pass 175, Batch 0, Cost 0.015014\n",
      "Test 175, Cost 0.006851\n",
      "Pass 176, Batch 0, Cost 0.016135\n",
      "Test 176, Cost 0.008143\n",
      "Pass 177, Batch 0, Cost 0.015527\n",
      "Test 177, Cost 0.010030\n",
      "Pass 178, Batch 0, Cost 0.016643\n",
      "Test 178, Cost 0.011916\n",
      "Pass 179, Batch 0, Cost 0.015942\n",
      "Test 179, Cost 0.012949\n",
      "Pass 180, Batch 0, Cost 0.015209\n",
      "Test 180, Cost 0.012317\n",
      "Pass 181, Batch 0, Cost 0.014665\n",
      "Test 181, Cost 0.009546\n",
      "Pass 182, Batch 0, Cost 0.016336\n",
      "Test 182, Cost 0.006582\n",
      "Pass 183, Batch 0, Cost 0.015488\n",
      "Test 183, Cost 0.005203\n",
      "Pass 184, Batch 0, Cost 0.016344\n",
      "Test 184, Cost 0.005214\n",
      "Pass 185, Batch 0, Cost 0.016626\n",
      "Test 185, Cost 0.005380\n",
      "Pass 186, Batch 0, Cost 0.017490\n",
      "Test 186, Cost 0.005427\n",
      "Pass 187, Batch 0, Cost 0.016188\n",
      "Test 187, Cost 0.005303\n",
      "Pass 188, Batch 0, Cost 0.015434\n",
      "Test 188, Cost 0.004836\n",
      "Pass 189, Batch 0, Cost 0.014943\n",
      "Test 189, Cost 0.004493\n",
      "Pass 190, Batch 0, Cost 0.015815\n",
      "Test 190, Cost 0.004227\n",
      "Pass 191, Batch 0, Cost 0.016163\n",
      "Test 191, Cost 0.004303\n",
      "Pass 192, Batch 0, Cost 0.016536\n",
      "Test 192, Cost 0.004842\n",
      "Pass 193, Batch 0, Cost 0.014827\n",
      "Test 193, Cost 0.005683\n",
      "Pass 194, Batch 0, Cost 0.014893\n",
      "Test 194, Cost 0.006942\n",
      "Pass 195, Batch 0, Cost 0.014744\n",
      "Test 195, Cost 0.008332\n",
      "Pass 196, Batch 0, Cost 0.016023\n",
      "Test 196, Cost 0.009595\n",
      "Pass 197, Batch 0, Cost 0.016642\n",
      "Test 197, Cost 0.009835\n",
      "Pass 198, Batch 0, Cost 0.016422\n",
      "Test 198, Cost 0.008890\n",
      "Pass 199, Batch 0, Cost 0.013980\n",
      "Test 199, Cost 0.006890\n",
      "Pass 200, Batch 0, Cost 0.015709\n",
      "Test 200, Cost 0.005458\n",
      "Pass 201, Batch 0, Cost 0.015280\n",
      "Test 201, Cost 0.004668\n",
      "Pass 202, Batch 0, Cost 0.015098\n",
      "Test 202, Cost 0.004358\n",
      "Pass 203, Batch 0, Cost 0.016174\n",
      "Test 203, Cost 0.004291\n",
      "Pass 204, Batch 0, Cost 0.015266\n",
      "Test 204, Cost 0.004298\n",
      "Pass 205, Batch 0, Cost 0.014693\n",
      "Test 205, Cost 0.004421\n",
      "Pass 206, Batch 0, Cost 0.014378\n",
      "Test 206, Cost 0.004610\n",
      "Pass 207, Batch 0, Cost 0.015166\n",
      "Test 207, Cost 0.004972\n",
      "Pass 208, Batch 0, Cost 0.014399\n",
      "Test 208, Cost 0.005362\n",
      "Pass 209, Batch 0, Cost 0.015665\n",
      "Test 209, Cost 0.005874\n",
      "Pass 210, Batch 0, Cost 0.015541\n",
      "Test 210, Cost 0.006234\n",
      "Pass 211, Batch 0, Cost 0.015589\n",
      "Test 211, Cost 0.006662\n",
      "Pass 212, Batch 0, Cost 0.015166\n",
      "Test 212, Cost 0.006716\n",
      "Pass 213, Batch 0, Cost 0.015128\n",
      "Test 213, Cost 0.006679\n",
      "Pass 214, Batch 0, Cost 0.014624\n",
      "Test 214, Cost 0.006365\n",
      "Pass 215, Batch 0, Cost 0.016926\n",
      "Test 215, Cost 0.006036\n",
      "Pass 216, Batch 0, Cost 0.015114\n",
      "Test 216, Cost 0.005460\n",
      "Pass 217, Batch 0, Cost 0.015909\n",
      "Test 217, Cost 0.005233\n",
      "Pass 218, Batch 0, Cost 0.014342\n",
      "Test 218, Cost 0.005011\n",
      "Pass 219, Batch 0, Cost 0.015847\n",
      "Test 219, Cost 0.004848\n",
      "Pass 220, Batch 0, Cost 0.013885\n",
      "Test 220, Cost 0.004786\n",
      "Pass 221, Batch 0, Cost 0.014730\n",
      "Test 221, Cost 0.004818\n",
      "Pass 222, Batch 0, Cost 0.015735\n",
      "Test 222, Cost 0.004941\n",
      "Pass 223, Batch 0, Cost 0.013664\n",
      "Test 223, Cost 0.005050\n",
      "Pass 224, Batch 0, Cost 0.014239\n",
      "Test 224, Cost 0.005262\n",
      "Pass 225, Batch 0, Cost 0.015114\n",
      "Test 225, Cost 0.005564\n",
      "Pass 226, Batch 0, Cost 0.015599\n",
      "Test 226, Cost 0.005771\n",
      "Pass 227, Batch 0, Cost 0.014370\n",
      "Test 227, Cost 0.005932\n",
      "Pass 228, Batch 0, Cost 0.013756\n",
      "Test 228, Cost 0.006075\n",
      "Pass 229, Batch 0, Cost 0.015411\n",
      "Test 229, Cost 0.006231\n",
      "Pass 230, Batch 0, Cost 0.015371\n",
      "Test 230, Cost 0.006006\n",
      "Pass 231, Batch 0, Cost 0.014291\n",
      "Test 231, Cost 0.005755\n",
      "Pass 232, Batch 0, Cost 0.014849\n",
      "Test 232, Cost 0.005503\n",
      "Pass 233, Batch 0, Cost 0.015147\n",
      "Test 233, Cost 0.005339\n",
      "Pass 234, Batch 0, Cost 0.014260\n",
      "Test 234, Cost 0.005195\n",
      "Pass 235, Batch 0, Cost 0.014495\n",
      "Test 235, Cost 0.005086\n",
      "Pass 236, Batch 0, Cost 0.014324\n",
      "Test 236, Cost 0.005052\n",
      "Pass 237, Batch 0, Cost 0.014692\n",
      "Test 237, Cost 0.005040\n",
      "Pass 238, Batch 0, Cost 0.014182\n",
      "Test 238, Cost 0.005086\n",
      "Pass 239, Batch 0, Cost 0.014697\n",
      "Test 239, Cost 0.005168\n",
      "Pass 240, Batch 0, Cost 0.015006\n",
      "Test 240, Cost 0.005314\n",
      "Pass 241, Batch 0, Cost 0.014378\n",
      "Test 241, Cost 0.005483\n",
      "Pass 242, Batch 0, Cost 0.013995\n",
      "Test 242, Cost 0.005501\n",
      "Pass 243, Batch 0, Cost 0.013802\n",
      "Test 243, Cost 0.005691\n",
      "Pass 244, Batch 0, Cost 0.014550\n",
      "Test 244, Cost 0.005706\n",
      "Pass 245, Batch 0, Cost 0.013577\n",
      "Test 245, Cost 0.005634\n",
      "Pass 246, Batch 0, Cost 0.014235\n",
      "Test 246, Cost 0.005625\n",
      "Pass 247, Batch 0, Cost 0.013772\n",
      "Test 247, Cost 0.005496\n",
      "Pass 248, Batch 0, Cost 0.014715\n",
      "Test 248, Cost 0.005496\n",
      "Pass 249, Batch 0, Cost 0.013806\n",
      "Test 249, Cost 0.005370\n",
      "Pass 250, Batch 0, Cost 0.014340\n",
      "Test 250, Cost 0.005301\n",
      "Pass 251, Batch 0, Cost 0.015306\n",
      "Test 251, Cost 0.005217\n",
      "Pass 252, Batch 0, Cost 0.014367\n",
      "Test 252, Cost 0.005250\n",
      "Pass 253, Batch 0, Cost 0.014127\n",
      "Test 253, Cost 0.005203\n",
      "Pass 254, Batch 0, Cost 0.014876\n",
      "Test 254, Cost 0.005308\n",
      "Pass 255, Batch 0, Cost 0.015322\n",
      "Test 255, Cost 0.005396\n",
      "Pass 256, Batch 0, Cost 0.012929\n",
      "Test 256, Cost 0.005355\n",
      "Pass 257, Batch 0, Cost 0.014577\n",
      "Test 257, Cost 0.005457\n",
      "Pass 258, Batch 0, Cost 0.014042\n",
      "Test 258, Cost 0.005439\n",
      "Pass 259, Batch 0, Cost 0.015280\n",
      "Test 259, Cost 0.005522\n",
      "Pass 260, Batch 0, Cost 0.015411\n",
      "Test 260, Cost 0.005440\n",
      "Pass 261, Batch 0, Cost 0.013699\n",
      "Test 261, Cost 0.005296\n",
      "Pass 262, Batch 0, Cost 0.013571\n",
      "Test 262, Cost 0.005243\n",
      "Pass 263, Batch 0, Cost 0.015554\n",
      "Test 263, Cost 0.005233\n",
      "Pass 264, Batch 0, Cost 0.014824\n",
      "Test 264, Cost 0.005133\n",
      "Pass 265, Batch 0, Cost 0.013321\n",
      "Test 265, Cost 0.005052\n",
      "Pass 266, Batch 0, Cost 0.013484\n",
      "Test 266, Cost 0.005088\n",
      "Pass 267, Batch 0, Cost 0.013858\n",
      "Test 267, Cost 0.005033\n",
      "Pass 268, Batch 0, Cost 0.014385\n",
      "Test 268, Cost 0.005037\n",
      "Pass 269, Batch 0, Cost 0.014755\n",
      "Test 269, Cost 0.005040\n",
      "Pass 270, Batch 0, Cost 0.013908\n",
      "Test 270, Cost 0.005128\n",
      "Pass 271, Batch 0, Cost 0.014730\n",
      "Test 271, Cost 0.005132\n",
      "Pass 272, Batch 0, Cost 0.013873\n",
      "Test 272, Cost 0.005184\n",
      "Pass 273, Batch 0, Cost 0.013187\n",
      "Test 273, Cost 0.005385\n",
      "Pass 274, Batch 0, Cost 0.013657\n",
      "Test 274, Cost 0.005565\n",
      "Pass 275, Batch 0, Cost 0.013730\n",
      "Test 275, Cost 0.005704\n",
      "Pass 276, Batch 0, Cost 0.014282\n",
      "Test 276, Cost 0.005781\n",
      "Pass 277, Batch 0, Cost 0.013943\n",
      "Test 277, Cost 0.005764\n",
      "Pass 278, Batch 0, Cost 0.014079\n",
      "Test 278, Cost 0.005778\n",
      "Pass 279, Batch 0, Cost 0.014842\n",
      "Test 279, Cost 0.005700\n",
      "Pass 280, Batch 0, Cost 0.016043\n",
      "Test 280, Cost 0.005617\n",
      "Pass 281, Batch 0, Cost 0.014785\n",
      "Test 281, Cost 0.005409\n",
      "Pass 282, Batch 0, Cost 0.015530\n",
      "Test 282, Cost 0.005362\n",
      "Pass 283, Batch 0, Cost 0.015136\n",
      "Test 283, Cost 0.005133\n",
      "Pass 284, Batch 0, Cost 0.013260\n",
      "Test 284, Cost 0.004882\n",
      "Pass 285, Batch 0, Cost 0.013405\n",
      "Test 285, Cost 0.004745\n",
      "Pass 286, Batch 0, Cost 0.014584\n",
      "Test 286, Cost 0.004708\n",
      "Pass 287, Batch 0, Cost 0.014181\n",
      "Test 287, Cost 0.004668\n",
      "Pass 288, Batch 0, Cost 0.013813\n",
      "Test 288, Cost 0.004632\n",
      "Pass 289, Batch 0, Cost 0.013252\n",
      "Test 289, Cost 0.004638\n",
      "Pass 290, Batch 0, Cost 0.013921\n",
      "Test 290, Cost 0.004777\n",
      "Pass 291, Batch 0, Cost 0.013679\n",
      "Test 291, Cost 0.004825\n",
      "Pass 292, Batch 0, Cost 0.014050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 292, Cost 0.005011\n",
      "Pass 293, Batch 0, Cost 0.012998\n",
      "Test 293, Cost 0.005154\n",
      "Pass 294, Batch 0, Cost 0.014003\n",
      "Test 294, Cost 0.005388\n",
      "Pass 295, Batch 0, Cost 0.014981\n",
      "Test 295, Cost 0.005737\n",
      "Pass 296, Batch 0, Cost 0.013855\n",
      "Test 296, Cost 0.005993\n",
      "Pass 297, Batch 0, Cost 0.014237\n",
      "Test 297, Cost 0.006085\n",
      "Pass 298, Batch 0, Cost 0.013762\n",
      "Test 298, Cost 0.006132\n",
      "Pass 299, Batch 0, Cost 0.015016\n",
      "Test 299, Cost 0.006163\n",
      "Pass 300, Batch 0, Cost 0.014648\n",
      "Test 300, Cost 0.006017\n",
      "Pass 301, Batch 0, Cost 0.013445\n",
      "Test 301, Cost 0.005578\n",
      "Pass 302, Batch 0, Cost 0.013583\n",
      "Test 302, Cost 0.005295\n",
      "Pass 303, Batch 0, Cost 0.014613\n",
      "Test 303, Cost 0.004983\n",
      "Pass 304, Batch 0, Cost 0.013699\n",
      "Test 304, Cost 0.004840\n",
      "Pass 305, Batch 0, Cost 0.014116\n",
      "Test 305, Cost 0.004680\n",
      "Pass 306, Batch 0, Cost 0.013861\n",
      "Test 306, Cost 0.004505\n",
      "Pass 307, Batch 0, Cost 0.013699\n",
      "Test 307, Cost 0.004454\n",
      "Pass 308, Batch 0, Cost 0.013794\n",
      "Test 308, Cost 0.004439\n",
      "Pass 309, Batch 0, Cost 0.013881\n",
      "Test 309, Cost 0.004423\n",
      "Pass 310, Batch 0, Cost 0.014006\n",
      "Test 310, Cost 0.004470\n",
      "Pass 311, Batch 0, Cost 0.013424\n",
      "Test 311, Cost 0.004565\n",
      "Pass 312, Batch 0, Cost 0.013399\n",
      "Test 312, Cost 0.004719\n",
      "Pass 313, Batch 0, Cost 0.015298\n",
      "Test 313, Cost 0.005015\n",
      "Pass 314, Batch 0, Cost 0.013688\n",
      "Test 314, Cost 0.005212\n",
      "Pass 315, Batch 0, Cost 0.013919\n",
      "Test 315, Cost 0.005531\n",
      "Pass 316, Batch 0, Cost 0.015205\n",
      "Test 316, Cost 0.005956\n",
      "Pass 317, Batch 0, Cost 0.013876\n",
      "Test 317, Cost 0.006033\n",
      "Pass 318, Batch 0, Cost 0.014090\n",
      "Test 318, Cost 0.006237\n",
      "Pass 319, Batch 0, Cost 0.012472\n",
      "Test 319, Cost 0.006197\n",
      "Pass 320, Batch 0, Cost 0.014282\n",
      "Test 320, Cost 0.006161\n",
      "Pass 321, Batch 0, Cost 0.013492\n",
      "Test 321, Cost 0.005921\n",
      "Pass 322, Batch 0, Cost 0.013309\n",
      "Test 322, Cost 0.005601\n",
      "Pass 323, Batch 0, Cost 0.013802\n",
      "Test 323, Cost 0.005209\n",
      "Pass 324, Batch 0, Cost 0.013709\n",
      "Test 324, Cost 0.004930\n",
      "Pass 325, Batch 0, Cost 0.014470\n",
      "Test 325, Cost 0.004656\n",
      "Pass 326, Batch 0, Cost 0.014580\n",
      "Test 326, Cost 0.004472\n",
      "Pass 327, Batch 0, Cost 0.014110\n",
      "Test 327, Cost 0.004369\n",
      "Pass 328, Batch 0, Cost 0.013187\n",
      "Test 328, Cost 0.004249\n",
      "Pass 329, Batch 0, Cost 0.013550\n",
      "Test 329, Cost 0.004226\n",
      "Pass 330, Batch 0, Cost 0.014336\n",
      "Test 330, Cost 0.004283\n",
      "Pass 331, Batch 0, Cost 0.013552\n",
      "Test 331, Cost 0.004305\n",
      "Pass 332, Batch 0, Cost 0.013903\n",
      "Test 332, Cost 0.004392\n",
      "Pass 333, Batch 0, Cost 0.013311\n",
      "Test 333, Cost 0.004541\n",
      "Pass 334, Batch 0, Cost 0.013819\n",
      "Test 334, Cost 0.004787\n",
      "Pass 335, Batch 0, Cost 0.014337\n",
      "Test 335, Cost 0.005012\n",
      "Pass 336, Batch 0, Cost 0.013160\n",
      "Test 336, Cost 0.005200\n",
      "Pass 337, Batch 0, Cost 0.015062\n",
      "Test 337, Cost 0.005717\n",
      "Pass 338, Batch 0, Cost 0.014061\n",
      "Test 338, Cost 0.005941\n",
      "Pass 339, Batch 0, Cost 0.013172\n",
      "Test 339, Cost 0.006102\n",
      "Pass 340, Batch 0, Cost 0.013654\n",
      "Test 340, Cost 0.006268\n",
      "Pass 341, Batch 0, Cost 0.014555\n",
      "Test 341, Cost 0.006514\n",
      "Pass 342, Batch 0, Cost 0.014084\n",
      "Test 342, Cost 0.006217\n",
      "Pass 343, Batch 0, Cost 0.014512\n",
      "Test 343, Cost 0.006018\n",
      "Pass 344, Batch 0, Cost 0.013542\n",
      "Test 344, Cost 0.005625\n",
      "Pass 345, Batch 0, Cost 0.013607\n",
      "Test 345, Cost 0.005274\n",
      "Pass 346, Batch 0, Cost 0.014111\n",
      "Test 346, Cost 0.004942\n",
      "Pass 347, Batch 0, Cost 0.014211\n",
      "Test 347, Cost 0.004661\n",
      "Pass 348, Batch 0, Cost 0.013920\n",
      "Test 348, Cost 0.004332\n",
      "Pass 349, Batch 0, Cost 0.013142\n",
      "Test 349, Cost 0.004231\n",
      "Pass 350, Batch 0, Cost 0.014002\n",
      "Test 350, Cost 0.004105\n",
      "Pass 351, Batch 0, Cost 0.012973\n",
      "Test 351, Cost 0.004022\n",
      "Pass 352, Batch 0, Cost 0.014974\n",
      "Test 352, Cost 0.004046\n",
      "Pass 353, Batch 0, Cost 0.014819\n",
      "Test 353, Cost 0.003989\n",
      "Pass 354, Batch 0, Cost 0.014918\n",
      "Test 354, Cost 0.004100\n",
      "Pass 355, Batch 0, Cost 0.014406\n",
      "Test 355, Cost 0.004165\n",
      "Pass 356, Batch 0, Cost 0.013940\n",
      "Test 356, Cost 0.004256\n",
      "Pass 357, Batch 0, Cost 0.013485\n",
      "Test 357, Cost 0.004397\n",
      "Pass 358, Batch 0, Cost 0.013050\n",
      "Test 358, Cost 0.004678\n",
      "Pass 359, Batch 0, Cost 0.014290\n",
      "Test 359, Cost 0.004999\n",
      "Pass 360, Batch 0, Cost 0.012873\n",
      "Test 360, Cost 0.005367\n",
      "Pass 361, Batch 0, Cost 0.013900\n",
      "Test 361, Cost 0.005988\n",
      "Pass 362, Batch 0, Cost 0.013875\n",
      "Test 362, Cost 0.006378\n",
      "Pass 363, Batch 0, Cost 0.013866\n",
      "Test 363, Cost 0.006874\n",
      "Pass 364, Batch 0, Cost 0.013483\n",
      "Test 364, Cost 0.007320\n",
      "Pass 365, Batch 0, Cost 0.013163\n",
      "Test 365, Cost 0.007270\n",
      "Pass 366, Batch 0, Cost 0.014467\n",
      "Test 366, Cost 0.007209\n",
      "Pass 367, Batch 0, Cost 0.015132\n",
      "Test 367, Cost 0.006432\n",
      "Pass 368, Batch 0, Cost 0.013843\n",
      "Test 368, Cost 0.005746\n",
      "Pass 369, Batch 0, Cost 0.013018\n",
      "Test 369, Cost 0.005047\n",
      "Pass 370, Batch 0, Cost 0.012660\n",
      "Test 370, Cost 0.004444\n",
      "Pass 371, Batch 0, Cost 0.013813\n",
      "Test 371, Cost 0.004094\n",
      "Pass 372, Batch 0, Cost 0.012766\n",
      "Test 372, Cost 0.003901\n",
      "Pass 373, Batch 0, Cost 0.013892\n",
      "Test 373, Cost 0.003806\n",
      "Pass 374, Batch 0, Cost 0.013633\n",
      "Test 374, Cost 0.003729\n",
      "Pass 375, Batch 0, Cost 0.012833\n",
      "Test 375, Cost 0.003734\n",
      "Pass 376, Batch 0, Cost 0.013991\n",
      "Test 376, Cost 0.003689\n",
      "Pass 377, Batch 0, Cost 0.013746\n",
      "Test 377, Cost 0.003707\n",
      "Pass 378, Batch 0, Cost 0.013169\n",
      "Test 378, Cost 0.003726\n",
      "Pass 379, Batch 0, Cost 0.013401\n",
      "Test 379, Cost 0.003771\n",
      "Pass 380, Batch 0, Cost 0.014283\n",
      "Test 380, Cost 0.003870\n",
      "Pass 381, Batch 0, Cost 0.013176\n",
      "Test 381, Cost 0.003931\n",
      "Pass 382, Batch 0, Cost 0.014108\n",
      "Test 382, Cost 0.004245\n",
      "Pass 383, Batch 0, Cost 0.013834\n",
      "Test 383, Cost 0.004531\n",
      "Pass 384, Batch 0, Cost 0.014528\n",
      "Test 384, Cost 0.005085\n",
      "Pass 385, Batch 0, Cost 0.013296\n",
      "Test 385, Cost 0.005766\n",
      "Pass 386, Batch 0, Cost 0.013085\n",
      "Test 386, Cost 0.006445\n",
      "Pass 387, Batch 0, Cost 0.014712\n",
      "Test 387, Cost 0.007524\n",
      "Pass 388, Batch 0, Cost 0.014141\n",
      "Test 388, Cost 0.008592\n",
      "Pass 389, Batch 0, Cost 0.014126\n",
      "Test 389, Cost 0.009291\n",
      "Pass 390, Batch 0, Cost 0.014944\n",
      "Test 390, Cost 0.009315\n",
      "Pass 391, Batch 0, Cost 0.012487\n",
      "Test 391, Cost 0.008449\n",
      "Pass 392, Batch 0, Cost 0.012586\n",
      "Test 392, Cost 0.007032\n",
      "Pass 393, Batch 0, Cost 0.012819\n",
      "Test 393, Cost 0.005505\n",
      "Pass 394, Batch 0, Cost 0.013326\n",
      "Test 394, Cost 0.004490\n",
      "Pass 395, Batch 0, Cost 0.012521\n",
      "Test 395, Cost 0.003916\n",
      "Pass 396, Batch 0, Cost 0.012990\n",
      "Test 396, Cost 0.003713\n",
      "Pass 397, Batch 0, Cost 0.013317\n",
      "Test 397, Cost 0.003656\n",
      "Pass 398, Batch 0, Cost 0.013055\n",
      "Test 398, Cost 0.003645\n",
      "Pass 399, Batch 0, Cost 0.013306\n",
      "Test 399, Cost 0.003668\n"
     ]
    }
   ],
   "source": [
    "# Get training data set\n",
    "train_x, train_y = get_train_data(1497)\n",
    "\n",
    "# Start training\n",
    "trainer.train(\n",
    "    reader=paddle.batch(paddle.reader.shuffle(train_reader(train_x, train_y), buf_size=60), batch_size=30),\n",
    "    feeding=feeding,    \n",
    "    event_handler=event_handler,num_passes=400)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Make predictons and visulize the results\n",
    "We have finished the training process. And we can use the model to make predictions and evaluate the model.\n",
    "#### 2.6.1 Get the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = get_test_data(642)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2 Preprocess the test data set\n",
    "We also need to preprocess the test date like normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data[1497:, 0]\n",
    "maximums = data_test.max(axis=0)\n",
    "minimums = data_test.min(axis=0)\n",
    "avgs = data_test.sum(axis=0) / data_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.3 Start testing\n",
    "We use predict function ```paddle.infer(output_layer = y_predict,parameters = parameters ,input = [b],feeding=feeding)``` to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = []\n",
    "for i in range(642):\n",
    "    test_list = []\n",
    "    \n",
    "    # Transfer each group data in test_x into array.\n",
    "    test_list.append([list(test_x[i])])\n",
    "    test_list = np.array(test_list)\n",
    "    \n",
    "    # Get the predicted value.\n",
    "    probs = paddle.infer(output_layer = y_predict, parameters = parameters , input=[test_list], feeding=feeding)\n",
    "    \n",
    "    # Denormalize the predicted values and store them in list pre\n",
    "    probs = probs * (maximums - minimums) + avgs\n",
    "    pre.append(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.4 Use another list to store results for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []        # stores the predicted values\n",
    "for i in pre:\n",
    "    i = float(i)\n",
    "    pred.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.5 Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(list(range(len(pred))), pred, color='b')           # blue line represents the predicted values\n",
    "plt.plot(list(range(len(pred))), data_test, color='r')     # red line represents the real values\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.6 Evaluate the model\n",
    "We use Average Error to represent the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025414720918355484\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    acc.append(np.abs((pred[i] - data_test[i]) / data_test[i]))\n",
    "acc = np.average(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3C4B2E7E6994ACD86D0F87E78316940",
    "scrolled": false
   },
   "source": [
    "## 3 Acknowledgement \n",
    "- I would like to thank Hangjiweilaicloud for providing tutorials about Prediction of Stock Prices Based on LSTM. \n",
    "- Website: http://aistudio.baidu.com/?_=1538223849065#/projectdetail/21919"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.2.0 (Python 2.7)",
   "language": "python",
   "name": "py27-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
